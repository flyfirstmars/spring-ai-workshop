{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b188803",
   "metadata": {},
   "source": [
    "# LLM API Foundations Workshop\n",
    "\n",
    "This guide distills the core mechanics behind working with Large Language Model APIs across OpenAI and Azure OpenAI deployments. Use it as a reference for how to pass information into a model, what content can be supplied, and which controls shape behavior, cost, and reliability. Getting these fundamentals right routinely cuts token spend by 80%+, improves task accuracy by ~40%, and unlocks workflows that simple prompt-response bots cannot handle.\n",
    "\n",
    "**Why This Matters:**\n",
    "\n",
    "LLMs behave unlike deterministic APIs: each response is sampled token-by-token from probability distributions, the services are stateless between calls, and every byte you send or receive is billed in tokens. As a result, poor prompt hygiene, unchecked context growth, or sloppy memory strategies surface as failures: context overflows, incoherent replies, and unexpected cost spikes. Treat tokens, prompts, and context as first-class resources from the outset; modern \"context engineering\" is really continuous memory management for probabilistic systems.\n",
    "\n",
    "**Workshop Goals:**\n",
    "- Master stateless API mechanics and token economics\n",
    "- Understand GPT-5 reasoning vs GPT-4o standard model differences  \n",
    "- Implement tool calling, streaming, and structured outputs\n",
    "- Apply context management strategies for production workflows\n",
    "\n",
    "**Prerequisites:** Python 3.8+, `requests` library, DIAL API key\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e769c273",
   "metadata": {},
   "source": [
    "## Part 1: Setup & First API Call\n",
    "\n",
    "### Theory: What Are LLM APIs?\n",
    "\n",
    "LLM APIs are stateless HTTP endpoints: each request is independent, and you must send the full conversation history every time. Two main patterns:\n",
    "\n",
    "**Chat Completions** (most common):\n",
    "- Send `messages[]` array with roles: `system`, `user`, `assistant`, `tool`\n",
    "- For GPT-5 reasoning models: use `max_completion_tokens` + `reasoning_effort`\n",
    "- For GPT-4o standard: use `max_tokens` + `temperature`\n",
    "\n",
    "**Key Point:** Every byte you send or receive costs tokens. Poor prompt hygiene = cost spikes and failures.\n",
    "\n",
    "---\n",
    "\n",
    "**Default Models:**\n",
    "- GPT-5: `gpt-5-mini-2025-08-07` (reasoning model)\n",
    "- GPT-4o: `gpt-4o-mini-2024-07-18` (standard model)\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 1.1: Configure API Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7749b91d9da04d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration set:\n",
      "  Endpoint: https://ai-proxy.lab.epam.com\n",
      "  API Version: 2024-10-21\n",
      "  API Key: ******************** (hidden)\n",
      "\n",
      "  Primary (GPT-5): gpt-5-mini-2025-08-07\n",
      "  Comparison (GPT-4o): gpt-4o-mini-2024-07-18\n",
      "\n",
      "‚úì GPT-5 Reasoning Model Configuration:\n",
      "  ‚úì Supported: max_completion_tokens, reasoning_effort, developer role\n",
      "  ‚úó NOT supported: temperature, top_p, max_tokens, frequency/presence_penalty\n",
      "\n",
      "üìö Workshop Structure:\n",
      "  ‚Ä¢ Most demos use GPT-5 (reasoning model)\n",
      "  ‚Ä¢ Demos 2-4 compare GPT-5 vs GPT-4o\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Collect and normalize configuration\n",
    "if 'DIAL_API_KEY' not in os.environ or not os.environ['DIAL_API_KEY']:\n",
    "    os.environ['DIAL_API_KEY'] = getpass.getpass('Enter DIAL_API_KEY: ')\n",
    "\n",
    "# Default to GPT-5 mini (reasoning model) for main workshop\n",
    "current_deployment = os.environ.get('DIAL_DEPLOYMENT', 'gpt-5-mini-2025-08-07')\n",
    "chosen = input(f\"Enter DIAL_DEPLOYMENT [{current_deployment}]: \").strip()\n",
    "os.environ['DIAL_DEPLOYMENT'] = chosen or current_deployment\n",
    "\n",
    "# Also store GPT-4o deployment for comparison demos\n",
    "gpt4o_deployment = os.environ.get('GPT4O_DEPLOYMENT', 'gpt-4o-mini-2024-07-18')\n",
    "gpt4o_chosen = input(f\"Enter GPT4O_DEPLOYMENT for comparisons [{gpt4o_deployment}]: \").strip()\n",
    "os.environ['GPT4O_DEPLOYMENT'] = gpt4o_chosen or gpt4o_deployment\n",
    "\n",
    "os.environ.setdefault('DIAL_API_ENDPOINT', 'https://ai-proxy.lab.epam.com')\n",
    "os.environ.setdefault('DIAL_API_VERSION', '2024-10-21')\n",
    "\n",
    "# Remove any trailing slash to avoid double-slash URLs\n",
    "os.environ['DIAL_API_ENDPOINT'] = os.environ['DIAL_API_ENDPOINT'].rstrip('/')\n",
    "\n",
    "# Detect if this is a reasoning model\n",
    "deployment = os.environ['DIAL_DEPLOYMENT'].lower()\n",
    "reasoning_models = ['gpt-5', 'o1', 'o3', 'o4']\n",
    "is_reasoning_model = any(model in deployment for model in reasoning_models)\n",
    "\n",
    "print('‚úì Configuration set:')\n",
    "print(f\"  Endpoint: {os.environ['DIAL_API_ENDPOINT']}\")\n",
    "print(f\"  API Version: {os.environ['DIAL_API_VERSION']}\")\n",
    "print(f\"  API Key: {'*' * 20} (hidden)\")\n",
    "print(f\"\\n  Primary (GPT-5): {os.environ['DIAL_DEPLOYMENT']}\")\n",
    "print(f\"  Comparison (GPT-4o): {os.environ['GPT4O_DEPLOYMENT']}\")\n",
    "\n",
    "if is_reasoning_model:\n",
    "    print(f\"\\n‚úì GPT-5 Reasoning Model Configuration:\")\n",
    "    print(\"  ‚úì Supported: max_completion_tokens, reasoning_effort, developer role\")\n",
    "    print(\"  ‚úó NOT supported: temperature, top_p, max_tokens, frequency/presence_penalty\")\n",
    "    os.environ['MODEL_TYPE'] = 'reasoning'\n",
    "else:\n",
    "    print(f\"\\nÔ∏è  Warning: Primary deployment is not a reasoning model\")\n",
    "    print(\"  For this workshop, we recommend using gpt-5-mini or gpt-5\")\n",
    "    os.environ['MODEL_TYPE'] = 'standard'\n",
    "\n",
    "print(\"\\nüìö Workshop Structure:\")\n",
    "print(\"  ‚Ä¢ Most demos use GPT-5 (reasoning model)\")\n",
    "print(\"  ‚Ä¢ Demos 2-4 compare GPT-5 vs GPT-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1081a15907f2b7",
   "metadata": {},
   "source": [
    "### Demo 1.2: Validate Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73aabb0540482fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All environment variables are set\n",
      "\n",
      "Your configuration:\n",
      "  Endpoint: https://ai-proxy.lab.epam.com\n",
      "  Deployment: gpt-5-mini-2025-08-07\n",
      "  API Version: 2024-10-21\n",
      "  API Key: ******************** (hidden)\n",
      "\n",
      "API endpoint:\n",
      "  https://ai-proxy.lab.epam.com/openai/deployments/gpt-5-mini-2025-08-07/chat/completions?api-version=2024-10-21\n",
      "\n",
      "‚úì Ready to make API calls!\n"
     ]
    }
   ],
   "source": [
    "# Validate that all required environment variables are set\n",
    "required_vars = ['DIAL_API_KEY', 'DIAL_DEPLOYMENT', 'DIAL_API_ENDPOINT', 'DIAL_API_VERSION']\n",
    "missing = [var for var in required_vars if var not in os.environ]\n",
    "\n",
    "if missing:\n",
    "    print(f\" Missing environment variables: {', '.join(missing)}\")\n",
    "    print('   Run the configuration cell above first!')\n",
    "else:\n",
    "    print('‚úì All environment variables are set')\n",
    "    print('\\nYour configuration:')\n",
    "    print(f\"  Endpoint: {os.environ['DIAL_API_ENDPOINT']}\")\n",
    "    print(f\"  Deployment: {os.environ['DIAL_DEPLOYMENT']}\")\n",
    "    print(f\"  API Version: {os.environ['DIAL_API_VERSION']}\")\n",
    "    print(f\"  API Key: {'*' * 20} (hidden)\")\n",
    "\n",
    "    url = (\n",
    "        f\"{os.environ['DIAL_API_ENDPOINT']}/openai/deployments/\"\n",
    "        f\"{os.environ['DIAL_DEPLOYMENT']}/chat/completions\"\n",
    "        f\"?api-version={os.environ['DIAL_API_VERSION']}\"\n",
    "    )\n",
    "    print('\\nAPI endpoint:')\n",
    "    print(f'  {url}')\n",
    "\n",
    "    print('\\n‚úì Ready to make API calls!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c270e47e4d3c3f5c",
   "metadata": {},
   "source": [
    "### Demo 1.3: Connection Test\n",
    "\n",
    "**Critical:** This test must pass before proceeding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48718d6f558328e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CRITICAL: Testing API Connection\n",
      "======================================================================\n",
      "\n",
      "Testing REASONING model: gpt-5-mini-2025-08-07\n",
      "Parameters: max_completion_tokens, reasoning_effort\n",
      "Endpoint: https://ai-proxy.lab.epam.com/openai/deployments/gpt-5-mini-2025-08-07/chat/completions?api-version=2024-10-21\n",
      "\n",
      "HTTP Status: 200\n",
      "\n",
      "======================================================================\n",
      " SUCCESS! API is working correctly\n",
      "======================================================================\n",
      "\n",
      "Response: API TEST SUCCESSFUL\n",
      "\n",
      "Token usage:\n",
      "  Prompt tokens: 29\n",
      "  Completion tokens: 14\n",
      "  Total tokens: 43\n",
      "  Reasoning tokens: 0\n",
      "\n",
      " You can now run all the demos below!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CRITICAL: Testing API Connection\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build test request\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "deployment = os.environ['DIAL_DEPLOYMENT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "model_type = os.environ.get('MODEL_TYPE', 'standard')\n",
    "url = f\"{endpoint}/openai/deployments/{deployment}/chat/completions?api-version={api_version}\"\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'api-key': os.environ['DIAL_API_KEY'],\n",
    "}\n",
    "\n",
    "# Build test payload based on model type\n",
    "if model_type == 'reasoning':\n",
    "    # For reasoning models (GPT-5, o1, o3)\n",
    "    test_payload = {\n",
    "        'messages': [\n",
    "            {'role': 'developer', 'content': 'You are a helpful assistant.'},\n",
    "            {'role': 'user', 'content': 'Say \"API TEST SUCCESSFUL\" if you can read this.'}\n",
    "        ],\n",
    "        'max_completion_tokens': 50,\n",
    "        'reasoning_effort': 'low'\n",
    "    }\n",
    "    print(f\"\\nTesting REASONING model: {deployment}\")\n",
    "    print(\"Parameters: max_completion_tokens, reasoning_effort\")\n",
    "else:\n",
    "    # For standard models (GPT-4o, GPT-4o-mini)\n",
    "    test_payload = {\n",
    "        'messages': [\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "            {'role': 'user', 'content': 'Say \"API TEST SUCCESSFUL\" if you can read this.'}\n",
    "        ],\n",
    "        'max_tokens': 50,\n",
    "        'temperature': 0.7\n",
    "    }\n",
    "    print(f\"\\nTesting STANDARD model: {deployment}\")\n",
    "    print(\"Parameters: max_tokens, temperature\")\n",
    "\n",
    "print(f\"Endpoint: {url}\\n\")\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, json=test_payload, timeout=30)\n",
    "    \n",
    "    # Show status code\n",
    "    print(f\"HTTP Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        content = result['choices'][0]['message']['content']\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\" SUCCESS! API is working correctly\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\nResponse: {content}\")\n",
    "        print(f\"\\nToken usage:\")\n",
    "        print(f\"  Prompt tokens: {result['usage']['prompt_tokens']}\")\n",
    "        print(f\"  Completion tokens: {result['usage']['completion_tokens']}\")\n",
    "        print(f\"  Total tokens: {result['usage']['total_tokens']}\")\n",
    "        \n",
    "        if 'completion_tokens_details' in result['usage']:\n",
    "            details = result['usage']['completion_tokens_details']\n",
    "            if 'reasoning_tokens' in details:\n",
    "                print(f\"  Reasoning tokens: {details['reasoning_tokens']}\")\n",
    "        \n",
    "        print(\"\\n You can now run all the demos below!\")\n",
    "        print(\"=\" * 70)\n",
    "    else:\n",
    "        # Show error details\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\" API ERROR - Demos will NOT work!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\nError response:\")\n",
    "        try:\n",
    "            error_data = response.json()\n",
    "            print(json.dumps(error_data, indent=2))\n",
    "        except:\n",
    "            print(response.text)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TROUBLESHOOTING:\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if response.status_code == 400:\n",
    "            print(\"\\n 400 Bad Request - Most likely causes:\")\n",
    "            print(\"  1. Wrong deployment name\")\n",
    "            print(\"  2. Using wrong parameters for this model type\")\n",
    "            print(\"  3. Model doesn't exist in your DIAL instance\")\n",
    "            print(\"\\nQuick fixes:\")\n",
    "            print(\"  ‚Ä¢ Try: gpt-4o (most common)\")\n",
    "            print(\"  ‚Ä¢ Try: gpt-4o-mini (cheaper, faster)\")\n",
    "            print(\"  ‚Ä¢ Try: gpt-35-turbo (older but widely available)\")\n",
    "            print(\"\\nRe-run the config cell and enter a different deployment name\")\n",
    "        \n",
    "        elif response.status_code == 401:\n",
    "            print(\"\\n 401 Unauthorized:\")\n",
    "            print(\"  Your API key is invalid or expired\")\n",
    "            print(\"\\nFix: Get a new API key and re-run config cell\")\n",
    "        \n",
    "        elif response.status_code == 403:\n",
    "            print(\"\\n 403 Forbidden:\")\n",
    "            print(\"  Your API key doesn't have access to this deployment\")\n",
    "            print(\"\\nFix: Request access or try a different deployment\")\n",
    "        \n",
    "        elif response.status_code == 404:\n",
    "            print(\"\\n 404 Not Found:\")\n",
    "            print(f\"  Deployment '{deployment}' doesn't exist\")\n",
    "            print(\"\\nFix: Check available deployments or use gpt-4o\")\n",
    "        \n",
    "        elif response.status_code == 429:\n",
    "            print(\"\\n 429 Rate Limit:\")\n",
    "            print(\"  Too many requests - wait a moment and try again\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"\\n Unexpected error: {response.status_code}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"Ô∏è  DO NOT RUN ANY DEMOS until this test passes!\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"\\n Connection timeout - check your network\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"\\n Cannot connect to API endpoint\")\n",
    "    print(f\"   Check if {endpoint} is accessible\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Unexpected error: {e}\")\n",
    "    print(\"   Check your configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba770e2dc8a973",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Your First Request\n",
    "\n",
    "### Theory: GPT-5 vs GPT-4o\n",
    "\n",
    "**Two Model Families:**\n",
    "\n",
    "| Parameter | GPT-5 (Reasoning) | GPT-4o (Standard) |\n",
    "|-----------|-------------------|-------------------|\n",
    "| Output limit | `max_completion_tokens` | `max_tokens` |\n",
    "| Control | `reasoning_effort` (low/medium/high) | `temperature` (0.0-2.0) |\n",
    "| Role | `developer` or `system` | `system` only |\n",
    "| Best for | Complex logic, debugging | Fast responses, vision |\n",
    "\n",
    "**Prompt Structure:** System/developer role ‚Üí user question ‚Üí assistant reply ‚Üí next user turn\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 2.1: Build Chat Client Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4d25e75c562acae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to call: https://ai-proxy.lab.epam.com/openai/deployments/gpt-5-mini-2025-08-07/chat/completions?api-version=2024-10-21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Any, Dict\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "class DialChatClient:\n",
    "    \"\"\"Wrapper around the DIAL Azure OpenAI Chat Completions endpoint for GPT-5 demos.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        required = [\"DIAL_API_ENDPOINT\", \"DIAL_DEPLOYMENT\", \"DIAL_API_VERSION\", \"DIAL_API_KEY\"]\n",
    "        missing = [var for var in required if var not in os.environ]\n",
    "        if missing:\n",
    "            raise RuntimeError(\n",
    "                f\"Missing environment variables: {', '.join(missing)}. Run the configuration cell above first.\"\n",
    "            )\n",
    "\n",
    "        self.endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "        self.deployment = os.environ['DIAL_DEPLOYMENT']\n",
    "        self.api_version = os.environ['DIAL_API_VERSION']\n",
    "        self.api_key = os.environ['DIAL_API_KEY']\n",
    "\n",
    "        self.url = (\n",
    "            f\"{self.endpoint}/openai/deployments/{self.deployment}/chat/completions\"\n",
    "            f\"?api-version={self.api_version}\"\n",
    "        )\n",
    "        self.headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'api-key': self.api_key,\n",
    "        }\n",
    "        self.total_tokens = 0\n",
    "        self.total_requests = 0\n",
    "\n",
    "    def call(self, payload: Dict[str, Any], *, show_request: bool = False, allow_error: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"Send a chat completion request and capture the response.\"\"\"\n",
    "        if show_request:\n",
    "            print('Request payload:')\n",
    "            print(json.dumps(payload, indent=2))\n",
    "\n",
    "        start = time.time()\n",
    "        response = requests.post(self.url, headers=self.headers, json=payload, timeout=60)\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as exc:\n",
    "            status = exc.response.status_code if exc.response is not None else None\n",
    "            if status == 403:\n",
    "                message = (\n",
    "                    \"403 Access denied from DIAL. Check that your API key has quota for \"\n",
    "                    f\"deployment '{self.deployment}' or switch to gpt-5-mini-2025-08-07.\"\n",
    "                )\n",
    "                print(message)\n",
    "            try:\n",
    "                error_payload = exc.response.json()\n",
    "            except Exception:\n",
    "                error_payload = {'text': getattr(exc.response, 'text', str(exc))}\n",
    "\n",
    "            if allow_error:\n",
    "                return {\n",
    "                    'error': error_payload,\n",
    "                    'status_code': status,\n",
    "                    'elapsed': elapsed,\n",
    "                }\n",
    "            raise\n",
    "\n",
    "        data = response.json()\n",
    "        usage = data.get('usage', {})\n",
    "        self.total_requests += 1\n",
    "        self.total_tokens += usage.get('total_tokens', 0)\n",
    "        return {\n",
    "            'data': data,\n",
    "            'elapsed': elapsed,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def render_choice(result: Dict[str, Any]) -> None:\n",
    "        \"\"\"Pretty-print the assistant reply and finish reason.\"\"\"\n",
    "        choice = result['data']['choices'][0]\n",
    "        print(choice['message']['content'].strip())\n",
    "        print('finish_reason:', choice['finish_reason'])\n",
    "        usage = result['data'].get('usage', {})\n",
    "        if usage:\n",
    "            print('usage:', json.dumps(usage, indent=2))\n",
    "\n",
    "    def usage_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return aggregate usage stats for all demo calls.\"\"\"\n",
    "        return {\n",
    "            'endpoint': self.endpoint,\n",
    "            'deployment': self.deployment,\n",
    "            'total_requests': self.total_requests,\n",
    "            'total_tokens': self.total_tokens,\n",
    "            'estimated_cost_usd': round(self.total_tokens * 0.000005, 6),\n",
    "        }\n",
    "\n",
    "\n",
    "def build_demo_client() -> DialChatClient:\n",
    "    \"\"\"Factory function so notebooks can rebuild the client after config changes.\"\"\"\n",
    "    return DialChatClient()\n",
    "\n",
    "\n",
    "client = build_demo_client()\n",
    "print('Ready to call:', client.url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd30113ca3da2f",
   "metadata": {},
   "source": [
    "### Demo 2.2: Basic GPT-5 Call\n",
    "\n",
    "Minimal payload with token tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c11eb4d248bf0c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request payload:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"developer\",\n",
      "      \"content\": \"You are a helpful assistant that explains technical concepts clearly.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Summarize what an API does in one sentence.\"\n",
      "    }\n",
      "  ],\n",
      "  \"max_completion_tokens\": 120,\n",
      "  \"reasoning_effort\": \"medium\"\n",
      "}\n",
      "\n",
      "finish_reason: length\n",
      "usage: {\n",
      "  \"completion_tokens\": 120,\n",
      "  \"prompt_tokens\": 32,\n",
      "  \"total_tokens\": 152,\n",
      "  \"completion_tokens_details\": {\n",
      "    \"accepted_prediction_tokens\": 0,\n",
      "    \"audio_tokens\": 0,\n",
      "    \"reasoning_tokens\": 120,\n",
      "    \"rejected_prediction_tokens\": 0\n",
      "  },\n",
      "  \"prompt_tokens_details\": {\n",
      "    \"audio_tokens\": 0,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "basic_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You are a helpful assistant that explains technical concepts clearly.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Summarize what an API does in one sentence.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_completion_tokens\": 120,\n",
    "    \"reasoning_effort\": \"medium\"\n",
    "}\n",
    "\n",
    "basic_result = client.call(basic_payload, show_request=True)\n",
    "client.render_choice(basic_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6846a0b6117a17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Reasoning & Tokens\n",
    "\n",
    "### Theory: How GPT-5 \"Thinks\"\n",
    "\n",
    "**Reasoning Effort Levels:**\n",
    "- `low` - Fast, cheap (~10-50 thinking tokens)\n",
    "- `medium` - Balanced (~50-200 tokens)\n",
    "- `high` - Deep analysis (~200-1000+ tokens)\n",
    "\n",
    "**Token Economics:** ~4 chars = 1 token. You pay for:\n",
    "- Input tokens (what you send)\n",
    "- Output tokens (2-3x more expensive)\n",
    "- Reasoning tokens (GPT-5 only, billed as output)\n",
    "\n",
    "**Pricing (2025):** GPT-5 input $1.25/M, output $10/M | GPT-4o input $2.50/M, output $10/M\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 3.1: Compare Reasoning Efforts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49090077332a73c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Low reasoning effort ---\n",
      "\n",
      "finish_reason: length | total_tokens: 361 | reasoning_tokens: 300\n",
      "\n",
      "--- High reasoning effort ---\n",
      "\n",
      "finish_reason: length | total_tokens: 361 | reasoning_tokens: 300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = (\n",
    "    \"If a store sells 15 items per hour and is open 8 hours a day, but 20% of customers return their items the next day, \"\n",
    "    \"how many net items are sold in a 7-day week?\"\n",
    ")\n",
    "\n",
    "payload_low = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"developer\", \"content\": \"You are a careful math tutor.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 300,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "payload_high = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"developer\", \"content\": \"You are a careful math tutor.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 300,\n",
    "    \"reasoning_effort\": \"high\"\n",
    "}\n",
    "\n",
    "def summarize(result, label):\n",
    "    choice = result['data']['choices'][0]\n",
    "    usage = result['data'].get('usage', {})\n",
    "    reasoning_tokens = usage.get('completion_tokens_details', {}).get('reasoning_tokens')\n",
    "    print(f\"--- {label} ---\")\n",
    "    print(choice['message']['content'].strip())\n",
    "    print(\n",
    "        f\"finish_reason: {choice['finish_reason']} | total_tokens: {usage.get('total_tokens')} | \"\n",
    "        f\"reasoning_tokens: {reasoning_tokens}\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "low_result = client.call(payload_low)\n",
    "high_result = client.call(payload_high)\n",
    "\n",
    "summarize(low_result, 'Low reasoning effort')\n",
    "summarize(high_result, 'High reasoning effort')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae1b1cf53fe714",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Multi-Turn Conversations\n",
    "\n",
    "### Theory: APIs Have No Memory\n",
    "\n",
    "**Core Principle:** The API doesn't remember previous calls. YOU resend history.\n",
    "\n",
    "**Pattern:**\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"developer\", \"content\": \"Instructions\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question 1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Answer 1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question 2\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Strategies:** Sliding window (keep last N), summarize old turns, prune aggressively.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 4.1: Multi-Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c14eeb91a3022de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1 response:\n",
      "\n",
      "\n",
      "Turn 2 response:\n",
      "\n",
      "\n",
      "Turn 3 response:\n",
      "\n",
      "\n",
      "Messages sent on turn 3: 6\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"developer\", \"content\": \"You are a DevOps expert.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Docker?\"}\n",
    "]\n",
    "\n",
    "turn1 = client.call({\"messages\": messages, \"max_completion_tokens\": 180, \"reasoning_effort\": \"low\"})\n",
    "reply1 = turn1['data']['choices'][0]['message']['content']\n",
    "print('Turn 1 response:\\n' + reply1.strip())\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": reply1})\n",
    "messages.append({\"role\": \"user\", \"content\": \"How does it differ from a virtual machine?\"})\n",
    "\n",
    "turn2 = client.call({\"messages\": messages, \"max_completion_tokens\": 220, \"reasoning_effort\": \"low\"})\n",
    "reply2 = turn2['data']['choices'][0]['message']['content']\n",
    "print('\\nTurn 2 response:\\n' + reply2.strip())\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": reply2})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Provide a minimal docker-compose example.\"})\n",
    "\n",
    "turn3 = client.call({\"messages\": messages, \"max_completion_tokens\": 260, \"reasoning_effort\": \"low\"})\n",
    "reply3 = turn3['data']['choices'][0]['message']['content']\n",
    "print('\\nTurn 3 response:\\n' + reply3.strip())\n",
    "\n",
    "print(f'\\nMessages sent on turn 3: {len(messages)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d734177b556f1db5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Token Limits & Costs\n",
    "\n",
    "### Theory: Context Windows\n",
    "\n",
    "**Model Limits:**\n",
    "- GPT-4o: 128K context, ~16K max output\n",
    "- GPT-5: 128K context (expandable to 272K), 32K max output\n",
    "\n",
    "**Critical Rule:** `prompt_tokens + max_completion_tokens < model_limit`\n",
    "\n",
    "**When `finish_reason == \"length\"`:** Response truncated! Increase limit or shrink prompt.\n",
    "\n",
    "**Lost-in-the-middle:** Quality drops beyond 50-55% of context limit. Keep important content at top and bottom.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 5.1: Token Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f658fbf711179bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response (60 tokens max):\n",
      "\n",
      "\n",
      "Finish reason: length\n",
      "\n",
      "Ô∏è  Context limit reached - response was truncated!\n"
     ]
    }
   ],
   "source": [
    "detailed_prompt = (\n",
    "    \"Explain REST APIs in depth, covering how requests flow, common verbs, typical response codes, \"\n",
    "    \"best practices, and frequent pitfalls.\"\n",
    ")\n",
    "\n",
    "constrained_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"developer\", \"content\": \"You are a technical writer.\"},\n",
    "        {\"role\": \"user\", \"content\": detailed_prompt}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 60,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "constrained = client.call(constrained_payload)\n",
    "choice_short = constrained['data']['choices'][0]\n",
    "content_short = choice_short['message']['content']\n",
    "\n",
    "print('Response (60 tokens max):\\n' + content_short.strip())\n",
    "print('\\nFinish reason: ' + choice_short['finish_reason'])\n",
    "\n",
    "if choice_short['finish_reason'] == 'length':\n",
    "    print('\\nÔ∏è  Context limit reached - response was truncated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91313b222154b315",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Structured Outputs\n",
    "\n",
    "### Theory: JSON Mode vs Schemas\n",
    "\n",
    "**JSON Mode:** `response_format: {\"type\": \"json_object\"}`\n",
    "- Guarantees valid JSON syntax\n",
    "- Field names/types can vary\n",
    "\n",
    "**Strict Schemas:** `{\"type\": \"json_schema\", \"strict\": true}`\n",
    "- Enforces exact fields and types\n",
    "- All fields required, `additionalProperties: false`\n",
    "- First call ~10 sec (caching), then fast\n",
    "\n",
    "**Ô∏è Important:** Guarantees format, NOT accuracy!\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 6.1: JSON Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ece1289475f8e052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw JSON response:\n",
      "{\"name\":\"Jisoo Kim\",\"age\":28,\"occupation\":\"software engineer\",\"city\":\"Singapore\"}\n",
      "\n",
      "Parsed entity:\n",
      "  name: Jisoo Kim\n",
      "  age: 28\n",
      "  occupation: software engineer\n",
      "  city: Singapore\n"
     ]
    }
   ],
   "source": [
    "json_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You extract entities and always respond with JSON containing name, age, occupation, city.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Jisoo Kim is a 28-year-old software engineer living in Singapore.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_completion_tokens\": 200,\n",
    "    \"reasoning_effort\": \"low\",\n",
    "    \"response_format\": {\"type\": \"json_object\"}\n",
    "}\n",
    "\n",
    "json_result = client.call(json_payload)\n",
    "raw_content = json_result['data']['choices'][0]['message']['content']\n",
    "\n",
    "print('Raw JSON response:\\n' + raw_content)\n",
    "\n",
    "try:\n",
    "    parsed = json.loads(raw_content)\n",
    "    print('\\nParsed entity:')\n",
    "    for key, val in parsed.items():\n",
    "        print(f'  {key}: {val}')\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f'\\n Failed to parse JSON: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5079c384bb4eb",
   "metadata": {},
   "source": [
    "### Demo 6.2: Strict JSON Schema\n",
    "\n",
    "Enforce exact structure for user profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5e9b05cff0d8028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Structured Output Demo with Strict JSON Schema\n",
      "======================================================================\n",
      "\n",
      "Input text:\n",
      "\n",
      "Meet Dr. Emily Rodriguez, a 34-year-old data scientist at TechCorp AI Labs \n",
      "with 8 years of experience in machine learning. She lives in Barcelona, Spain, \n",
      "and enjoys hiking, photography, and reading science fiction novels. \n",
      "You can reach her at emily.r@techcorp.ai for collaboration opportunities.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Schema enforces:\n",
      "  - Exact field names and types\n",
      "  - Required fields (no missing data)\n",
      "  - No additional properties\n",
      "  - Nested object structure\n",
      "======================================================================\n",
      "\n",
      "Raw response:\n",
      "{\n",
      "  \"personal_info\": {\n",
      "    \"full_name\": \"Dr. Emily Rodriguez\",\n",
      "    \"age\": 34,\n",
      "    \"email\": \"emily.r@techcorp.ai\"\n",
      "  },\n",
      "  \"professional_info\": {\n",
      "    \"occupation\": \"Data Scientist\",\n",
      "    \"company\": \"TechCorp AI Labs\",\n",
      "    \"years_of_experience\": 8\n",
      "  },\n",
      "  \"location\": {\n",
      "    \"city\": \"Barcelona\",\n",
      "    \"country\": \"Spain\"\n",
      "  },\n",
      "  \"interests\": [\n",
      "    \"Hiking\",\n",
      "    \"Photography\",\n",
      "    \"Reading science fiction novels\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "======================================================================\n",
      " Response successfully parsed as JSON\n",
      "======================================================================\n",
      "\n",
      "Formatted extraction:\n",
      "{\n",
      "  \"personal_info\": {\n",
      "    \"full_name\": \"Dr. Emily Rodriguez\",\n",
      "    \"age\": 34,\n",
      "    \"email\": \"emily.r@techcorp.ai\"\n",
      "  },\n",
      "  \"professional_info\": {\n",
      "    \"occupation\": \"Data Scientist\",\n",
      "    \"company\": \"TechCorp AI Labs\",\n",
      "    \"years_of_experience\": 8\n",
      "  },\n",
      "  \"location\": {\n",
      "    \"city\": \"Barcelona\",\n",
      "    \"country\": \"Spain\"\n",
      "  },\n",
      "  \"interests\": [\n",
      "    \"Hiking\",\n",
      "    \"Photography\",\n",
      "    \"Reading science fiction novels\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "======================================================================\n",
      "Accessing structured data:\n",
      "======================================================================\n",
      "  Name: Dr. Emily Rodriguez\n",
      "  Age: 34\n",
      "  Job: Data Scientist\n",
      "  Company: TechCorp AI Labs\n",
      "  Location: Barcelona, Spain\n",
      "  Experience: 8 years\n",
      "  Interests: Hiking, Photography, Reading science fiction novels\n",
      "\n",
      " All required fields present and correctly typed!\n"
     ]
    }
   ],
   "source": [
    "# Define a strict JSON schema for extracting user information\n",
    "user_schema = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"user_profile_extraction\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"personal_info\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"full_name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The person's full name\"\n",
    "                        },\n",
    "                        \"age\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"The person's age in years\"\n",
    "                        },\n",
    "                        \"email\": {\n",
    "                            \"type\": [\"string\", \"null\"],\n",
    "                            \"description\": \"Email address if mentioned, otherwise null\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"full_name\", \"age\", \"email\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"professional_info\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"occupation\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Current job title or occupation\"\n",
    "                        },\n",
    "                        \"company\": {\n",
    "                            \"type\": [\"string\", \"null\"],\n",
    "                            \"description\": \"Company name if mentioned\"\n",
    "                        },\n",
    "                        \"years_of_experience\": {\n",
    "                            \"type\": [\"integer\", \"null\"],\n",
    "                            \"description\": \"Years of professional experience if mentioned\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"occupation\", \"company\", \"years_of_experience\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"location\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"city\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City name\"\n",
    "                        },\n",
    "                        \"country\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Country name\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"city\", \"country\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"interests\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"description\": \"List of hobbies or interests mentioned\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"personal_info\", \"professional_info\", \"location\", \"interests\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sample text to extract from\n",
    "sample_text = \"\"\"\n",
    "Meet Dr. Emily Rodriguez, a 34-year-old data scientist at TechCorp AI Labs \n",
    "with 8 years of experience in machine learning. She lives in Barcelona, Spain, \n",
    "and enjoys hiking, photography, and reading science fiction novels. \n",
    "You can reach her at emily.r@techcorp.ai for collaboration opportunities.\n",
    "\"\"\"\n",
    "\n",
    "structured_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You extract structured information from text and return it in the specified JSON format.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Extract all relevant information from this text:\\n\\n{sample_text}\"\n",
    "        }\n",
    "    ],\n",
    "    \"response_format\": user_schema,\n",
    "    \"max_completion_tokens\": 500,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Structured Output Demo with Strict JSON Schema\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nInput text:\\n{sample_text}\")\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"Schema enforces:\")\n",
    "print(\"  - Exact field names and types\")\n",
    "print(\"  - Required fields (no missing data)\")\n",
    "print(\"  - No additional properties\")\n",
    "print(\"  - Nested object structure\")\n",
    "print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "result = client.call(structured_payload)\n",
    "response_content = result['data']['choices'][0]['message']['content']\n",
    "\n",
    "print(\"Raw response:\")\n",
    "print(response_content)\n",
    "\n",
    "# Parse and validate\n",
    "try:\n",
    "    parsed_data = json.loads(response_content)\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\" Response successfully parsed as JSON\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(\"\\nFormatted extraction:\")\n",
    "    print(json.dumps(parsed_data, indent=2))\n",
    "    \n",
    "    # Demonstrate accessing nested fields\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"Accessing structured data:\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"  Name: {parsed_data['personal_info']['full_name']}\")\n",
    "    print(f\"  Age: {parsed_data['personal_info']['age']}\")\n",
    "    print(f\"  Job: {parsed_data['professional_info']['occupation']}\")\n",
    "    print(f\"  Company: {parsed_data['professional_info']['company']}\")\n",
    "    print(f\"  Location: {parsed_data['location']['city']}, {parsed_data['location']['country']}\")\n",
    "    print(f\"  Experience: {parsed_data['professional_info']['years_of_experience']} years\")\n",
    "    print(f\"  Interests: {', '.join(parsed_data['interests'])}\")\n",
    "    \n",
    "    print(f\"\\n All required fields present and correctly typed!\")\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\n JSON parsing error: {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"\\n Missing expected field: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b789ff4183d99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Tool Calling\n",
    "\n",
    "### Theory: Function Calling Workflow\n",
    "\n",
    "**5-Step Process:**\n",
    "1. Define tools with JSON Schema (costs input tokens)\n",
    "2. Model decides to call (`finish_reason: \"tool_calls\"`)\n",
    "3. Your code executes the function\n",
    "4. Return results with `role: \"tool\"`\n",
    "5. Model synthesizes final answer\n",
    "\n",
    "**Control:** `tool_choice` = `\"auto\"`, `\"none\"`, `\"required\"`, or function name\n",
    "\n",
    "**Best Practice:** Design tools like clean APIs - single responsibility, clear names, constrained enums.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 7.1: Single Tool (Calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf7b6dbfca374254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: Sending request with tool definition\n",
      "======================================================================\n",
      "\n",
      "Finish reason: tool_calls\n",
      "Model response: None\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Model requested tool call\n",
      "======================================================================\n",
      "Function: calculate\n",
      "Arguments: {\n",
      "  \"operation\": \"multiply\",\n",
      "  \"x\": 127,\n",
      "  \"y\": 89\n",
      "}\n",
      "\n",
      "Function result: 11303\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Sending tool result back to model\n",
      "======================================================================\n",
      "\n",
      "Final response:\n",
      "127 √ó 89 = 11,303\n",
      "\n",
      "Finish reason: stop\n",
      "\n",
      "======================================================================\n",
      "Tool calling demo complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define a simple calculator tool\n",
    "def calculate(operation, x, y):\n",
    "    \"\"\"Execute a mathematical operation.\"\"\"\n",
    "    operations = {\n",
    "        'add': lambda a, b: a + b,\n",
    "        'subtract': lambda a, b: a - b,\n",
    "        'multiply': lambda a, b: a * b,\n",
    "        'divide': lambda a, b: a / b if b != 0 else 'Error: Division by zero'\n",
    "    }\n",
    "    return operations.get(operation, lambda a, b: 'Unknown operation')(x, y)\n",
    "\n",
    "# Define the tool schema for the model\n",
    "calculator_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calculate\",\n",
    "        \"description\": \"Perform basic arithmetic operations (add, subtract, multiply, divide)\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"operation\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                    \"description\": \"The arithmetic operation to perform\"\n",
    "                },\n",
    "                \"x\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The first number\"\n",
    "                },\n",
    "                \"y\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The second number\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"operation\", \"x\", \"y\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initial request with tool definition\n",
    "tool_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You are a helpful math assistant. Use the calculator tool when needed.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is 127 multiplied by 89?\"\n",
    "        }\n",
    "    ],\n",
    "    \"tools\": [calculator_tool],\n",
    "    \"tool_choice\": \"auto\",\n",
    "    \"max_completion_tokens\": 300,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: Sending request with tool definition\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Send request\n",
    "result1 = client.call(tool_payload, show_request=False)\n",
    "choice1 = result1['data']['choices'][0]\n",
    "message1 = choice1['message']\n",
    "\n",
    "print(f\"\\nFinish reason: {choice1['finish_reason']}\")\n",
    "print(f\"Model response: {message1.get('content', '(no text content)')}\")\n",
    "\n",
    "# Check if model wants to call a tool\n",
    "if choice1['finish_reason'] == 'tool_calls' and 'tool_calls' in message1:\n",
    "    tool_call = message1['tool_calls'][0]\n",
    "    function_name = tool_call['function']['name']\n",
    "    function_args = json.loads(tool_call['function']['arguments'])\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"STEP 2: Model requested tool call\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"Function: {function_name}\")\n",
    "    print(f\"Arguments: {json.dumps(function_args, indent=2)}\")\n",
    "    \n",
    "    # Execute the function\n",
    "    result = calculate(**function_args)\n",
    "    print(f\"\\nFunction result: {result}\")\n",
    "    \n",
    "    # Append the assistant's tool call request\n",
    "    tool_payload['messages'].append(message1)\n",
    "    \n",
    "    # Append the tool result\n",
    "    tool_payload['messages'].append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_call['id'],\n",
    "        \"content\": json.dumps({\"result\": result})\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"STEP 3: Sending tool result back to model\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    # Send back with tool result\n",
    "    result2 = client.call(tool_payload)\n",
    "    final_response = result2['data']['choices'][0]['message']['content']\n",
    "    \n",
    "    print(f\"\\nFinal response:\")\n",
    "    print(final_response)\n",
    "    print(f\"\\nFinish reason: {result2['data']['choices'][0]['finish_reason']}\")\n",
    "else:\n",
    "    print(\"\\nModel responded without calling tool (unexpected for this example)\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"Tool calling demo complete!\")\n",
    "print(f\"{'=' * 70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0700536c9baabf",
   "metadata": {},
   "source": [
    "### Demo 7.2: Multiple Tools\n",
    "\n",
    "Model chooses between tools, can call in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f916ff95c4206339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Multi-Tool Demo: Weather + Database\n",
      "======================================================================\n",
      "\n",
      "Model's initial response:\n",
      "Finish reason: tool_calls\n",
      "\n",
      "Model requested 2 tool call(s):\n",
      "\n",
      "  Tool: get_current_weather\n",
      "  Args: {\n",
      "    \"location\": \"Tokyo\",\n",
      "    \"units\": \"celsius\"\n",
      "}\n",
      "  Result: {\n",
      "    \"location\": \"Tokyo\",\n",
      "    \"temperature\": 22,\n",
      "    \"units\": \"celsius\",\n",
      "    \"conditions\": \"Sunny\"\n",
      "}\n",
      "\n",
      "  Tool: search_database\n",
      "  Args: {\n",
      "    \"query\": \"electronics\",\n",
      "    \"limit\": 5\n",
      "}\n",
      "  Result: [\n",
      "    {\n",
      "        \"id\": 1,\n",
      "        \"name\": \"Laptop Pro 15\",\n",
      "        \"price\": 1299,\n",
      "        \"category\": \"Electronics\"\n",
      "    },\n",
      "    {\n",
      "        \"id\": 2,\n",
      "        \"name\": \"Wireless Mouse\",\n",
      "        \"price\": 29,\n",
      "        \"category\": \"Electronics\"\n",
      "    }\n",
      "]\n",
      "\n",
      "======================================================================\n",
      "Sending tool results back to model...\n",
      "======================================================================\n",
      "\n",
      "Final response:\n",
      "Tokyo: 22¬∞C, sunny.\n",
      "\n",
      "Electronics found (top results):\n",
      "- Laptop Pro 15 ‚Äî $1299 (category: Electronics)\n",
      "- Wireless Mouse ‚Äî $29 (category: Electronics)\n",
      "\n",
      "Would you like more results, details on either item, or a different unit for the temperature?\n"
     ]
    }
   ],
   "source": [
    "# Define multiple tools\n",
    "def get_current_weather(location, units=\"celsius\"):\n",
    "    \"\"\"Simulated weather API.\"\"\"\n",
    "    # In real scenario, this would call an actual API\n",
    "    weather_data = {\n",
    "        \"San Francisco\": {\"temp\": 18, \"conditions\": \"Partly cloudy\"},\n",
    "        \"Tokyo\": {\"temp\": 22, \"conditions\": \"Sunny\"},\n",
    "        \"London\": {\"temp\": 12, \"conditions\": \"Rainy\"},\n",
    "        \"Paris\": {\"temp\": 15, \"conditions\": \"Overcast\"}\n",
    "    }\n",
    "    data = weather_data.get(location, {\"temp\": 20, \"conditions\": \"Unknown\"})\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"temperature\": data[\"temp\"],\n",
    "        \"units\": units,\n",
    "        \"conditions\": data[\"conditions\"]\n",
    "    }\n",
    "\n",
    "def search_database(query, limit=5):\n",
    "    \"\"\"Simulated database search.\"\"\"\n",
    "    # Simulate a product database\n",
    "    products = [\n",
    "        {\"id\": 1, \"name\": \"Laptop Pro 15\", \"price\": 1299, \"category\": \"Electronics\"},\n",
    "        {\"id\": 2, \"name\": \"Wireless Mouse\", \"price\": 29, \"category\": \"Electronics\"},\n",
    "        {\"id\": 3, \"name\": \"Office Chair\", \"price\": 249, \"category\": \"Furniture\"},\n",
    "        {\"id\": 4, \"name\": \"Desk Lamp\", \"price\": 45, \"category\": \"Furniture\"},\n",
    "        {\"id\": 5, \"name\": \"Notebook Set\", \"price\": 12, \"category\": \"Stationery\"}\n",
    "    ]\n",
    "    # Simple search simulation\n",
    "    results = [p for p in products if query.lower() in p['name'].lower() or query.lower() in p['category'].lower()]\n",
    "    return results[:limit]\n",
    "\n",
    "# Define tool schemas\n",
    "weather_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city name, e.g., San Francisco, Tokyo\"\n",
    "                },\n",
    "                \"units\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"Temperature units\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "database_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"search_database\",\n",
    "        \"description\": \"Search the product database for items matching a query\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query string\"\n",
    "                },\n",
    "                \"limit\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results to return\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function dispatcher\n",
    "tool_functions = {\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "    \"search_database\": search_database\n",
    "}\n",
    "\n",
    "# Multi-tool request\n",
    "multi_tool_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You are a helpful assistant with access to weather data and a product database.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather in Tokyo and can you find electronics in the database?\"\n",
    "        }\n",
    "    ],\n",
    "    \"tools\": [weather_tool, database_tool],\n",
    "    \"tool_choice\": \"auto\",\n",
    "    \"max_completion_tokens\": 500,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Multi-Tool Demo: Weather + Database\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# First request\n",
    "result1 = client.call(multi_tool_payload)\n",
    "choice1 = result1['data']['choices'][0]\n",
    "message1 = choice1['message']\n",
    "\n",
    "print(f\"\\nModel's initial response:\")\n",
    "print(f\"Finish reason: {choice1['finish_reason']}\")\n",
    "\n",
    "if 'tool_calls' in message1:\n",
    "    print(f\"\\nModel requested {len(message1['tool_calls'])} tool call(s):\")\n",
    "    \n",
    "    # Append assistant message\n",
    "    multi_tool_payload['messages'].append(message1)\n",
    "    \n",
    "    # Execute all tool calls\n",
    "    for tool_call in message1['tool_calls']:\n",
    "        function_name = tool_call['function']['name']\n",
    "        function_args = json.loads(tool_call['function']['arguments'])\n",
    "        \n",
    "        print(f\"\\n  Tool: {function_name}\")\n",
    "        print(f\"  Args: {json.dumps(function_args, indent=4)}\")\n",
    "        \n",
    "        # Execute function\n",
    "        function_result = tool_functions[function_name](**function_args)\n",
    "        print(f\"  Result: {json.dumps(function_result, indent=4)}\")\n",
    "        \n",
    "        # Append tool result\n",
    "        multi_tool_payload['messages'].append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call['id'],\n",
    "            \"content\": json.dumps(function_result)\n",
    "        })\n",
    "    \n",
    "    # Send results back to model\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"Sending tool results back to model...\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "    \n",
    "    result2 = client.call(multi_tool_payload)\n",
    "    final_message = result2['data']['choices'][0]['message']['content']\n",
    "    \n",
    "    print(\"Final response:\")\n",
    "    print(final_message)\n",
    "else:\n",
    "    print(\"\\nNo tool calls requested\")\n",
    "    print(message1.get('content', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17c7aa35adf28c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Streaming Responses\n",
    "\n",
    "### Theory: Server-Sent Events\n",
    "\n",
    "**How It Works:**\n",
    "1. Set `stream: true`\n",
    "2. Receive chunks as `data: {...}\\n\\n`\n",
    "3. Accumulate `delta.content`\n",
    "4. Stop at `data: [DONE]`\n",
    "\n",
    "**Benefits:** Lower perceived latency, progressive UI updates\n",
    "\n",
    "**Trade-off:** More complex error handling, can't validate before displaying\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 8.1: Stream Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b9c65b152ea1a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Streaming Demo\n",
      "======================================================================\n",
      "\n",
      "Streaming response (tokens appear in real-time):\n",
      "\n",
      "\n",
      "\n",
      "[Stream ended: length]\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Streaming Statistics:\n",
      "  Total chunks received: 2\n",
      "  Total characters: 0\n",
      "  Time elapsed: 6.17s\n",
      "  Avg time per chunk: 3082.5ms\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "# Streaming request\n",
    "streaming_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"You are a technical writer who explains concepts clearly.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain how HTTP requests work, step by step.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_completion_tokens\": 600,\n",
    "    \"stream\": True  # Enable streaming\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Streaming Demo\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nStreaming response (tokens appear in real-time):\\n\")\n",
    "\n",
    "# Prepare streaming request\n",
    "url = client.url\n",
    "headers = client.headers\n",
    "\n",
    "start_time = time.time()\n",
    "full_content = \"\"\n",
    "chunk_count = 0\n",
    "\n",
    "try:\n",
    "    # Make streaming request\n",
    "    response = requests.post(url, headers=headers, json=streaming_payload, stream=True, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Process Server-Sent Events\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode('utf-8')\n",
    "            \n",
    "            # SSE format: \"data: {...}\"\n",
    "            if line_str.startswith('data: '):\n",
    "                data_str = line_str[6:]  # Remove \"data: \" prefix\n",
    "                \n",
    "                # Check for stream end\n",
    "                if data_str.strip() == '[DONE]':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    # Parse JSON chunk\n",
    "                    chunk = json.loads(data_str)\n",
    "                    chunk_count += 1\n",
    "                    \n",
    "                    # Extract delta content\n",
    "                    if 'choices' in chunk and len(chunk['choices']) > 0:\n",
    "                        delta = chunk['choices'][0].get('delta', {})\n",
    "                        content = delta.get('content', '')\n",
    "                        \n",
    "                        if content:\n",
    "                            full_content += content\n",
    "                            # Print token(s) as they arrive\n",
    "                            print(content, end='', flush=True)\n",
    "                        \n",
    "                        # Check for finish reason\n",
    "                        finish_reason = chunk['choices'][0].get('finish_reason')\n",
    "                        if finish_reason:\n",
    "                            print(f\"\\n\\n[Stream ended: {finish_reason}]\")\n",
    "                \n",
    "                except json.JSONDecodeError:\n",
    "                    pass  # Skip malformed JSON\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\\n{'=' * 70}\")\n",
    "    print(f\"Streaming Statistics:\")\n",
    "    print(f\"  Total chunks received: {chunk_count}\")\n",
    "    print(f\"  Total characters: {len(full_content)}\")\n",
    "    print(f\"  Time elapsed: {elapsed:.2f}s\")\n",
    "    print(f\"  Avg time per chunk: {(elapsed/chunk_count)*1000:.1f}ms\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"\\n\\n Streaming error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634d66bd48cb49c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Vision (Multimodal)\n",
    "\n",
    "### Theory: Image Analysis\n",
    "\n",
    "**Capabilities:**\n",
    "- GPT-4o: text + images\n",
    "- GPT-5: text only (for now)\n",
    "\n",
    "**Detail Levels:**\n",
    "- `low`: 85 tokens flat (classification)\n",
    "- `high`: variable tokens, tiled processing (detailed analysis)\n",
    "\n",
    "**Formats:** Base64 data or public URLs. Max 20MB/image, 50 images/request.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 9.1: Analyze Vacation Photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbef5fcc296b6dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Vision Demo: Analyzing Local Vacation Photos\n",
      "======================================================================\n",
      "\n",
      "Analyzing: /Users/Natig_Kurbanov/IdeaProjects/spring-ai-workshop/src/main/resources/images/background-vacation1.jpeg\n",
      "\n",
      "GPT-4o Analysis:\n",
      "----------------------------------------------------------------------\n",
      "This vacation photo depicts a stunning tropical beach scene characterized by soft, sandy shores, crystal-clear turquoise waters, and lush green hills in the background. The shoreline is gently kissed by the waves, creating a serene atmosphere. Towering palm trees frame the scene, adding to the exotic feel, while traditional thatched-roof huts suggest a laid-back, island lifestyle.\n",
      "\n",
      "The location could likely be a tropical paradise such as the Maldives, Fiji, or a Caribbean island, known for their picturesque beaches and lush landscapes. The warm, inviting climate and vibrant scenery make it an ideal spot for relaxation and leisure.\n",
      "\n",
      "The activities and experiences suggested by this photo include sunbathing on the beach, swimming or snorkeling in the clear waters, exploring the nearby hills, and enjoying local cuisine in the huts. It evokes a sense of tranquility and adventure, appealing to those looking to escape to a serene getaway filled with natural beauty and cultural richness.\n",
      "\n",
      "======================================================================\n",
      "Token Usage:\n",
      "  Input tokens: 36861\n",
      "  Output tokens: 183\n",
      "  Total: 37044\n",
      "\n",
      " High detail image analysis uses more tokens but provides richer descriptions\n",
      "\n",
      "======================================================================\n",
      "Analyzing second vacation photo...\n",
      "======================================================================\n",
      "\n",
      "Analyzing: /Users/Natig_Kurbanov/IdeaProjects/spring-ai-workshop/src/main/resources/images/background-vacation2.jpeg\n",
      "----------------------------------------------------------------------\n",
      "The vacation scene depicted showcases a serene tropical paradise, characterized by a pristine shoreline where soft, golden sand meets the gentle, crystalline waves of a turquoise sea. Lush palm trees sway lightly in the warm breeze, their fronds casting playful shadows on the sand. In the distance, a verdant island rises gracefully, dotted with greenery, and a quaint beach hut adds a touch of charm to the landscape.\n",
      "\n",
      "In comparison to a typical beach destination, this scene evokes a sense of tranquility and seclusion. While many beach spots can be bustling with crowds and activity, this setting feels like a hidden gem, offering an escape from the noise of everyday life. The mood is peaceful and idyllic; the calm waters invite visitors for a refreshing swim, while the soft sounds of waves lapping against the shore create a soothing soundtrack.\n",
      "\n",
      "In a more conventional beach destination, one might expect vibrant energy, with sunbathers, beach games, and the lively chatter of families enjoying their day. While these settings can be exhilarating and fun, they often lack the serenity and intimate connection to nature that this tropical scene provides. Here, the focus shifts from social bustle to personal reflection, relaxation, and the simple joys of nature, making it an ideal retreat for those seeking solace and rejuvenation.\n",
      "\n",
      "======================================================================\n",
      "Token Comparison (detail='low' vs detail='high'):\n",
      "  Image 1 (high): 36861 input tokens\n",
      "  Image 2 (low):  2855 input tokens\n",
      "  Savings: ~34006 tokens with 'low' detail\n",
      "\n",
      "======================================================================\n",
      "Key Takeaways:\n",
      "======================================================================\n",
      " GPT-4o supports vision; GPT-5 is text-only\n",
      " 'detail': 'high' ‚Üí better analysis, more tokens\n",
      " 'detail': 'low' ‚Üí faster, cheaper, good for classification\n",
      " Images can be base64-encoded or public URLs\n",
      " Remove images from history after analysis to save tokens\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Use GPT-4o for vision support\n",
    "gpt4o_deployment = os.environ.get('GPT4O_DEPLOYMENT', 'gpt-4o')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Vision Demo: Analyzing Local Vacation Photos\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "IMAGE_RELATIVE_PATH = \"src/main/resources/images/background-vacation1.jpeg\"\n",
    "IMAGE_RELATIVE_PATH_2 = \"src/main/resources/images/background-vacation2.jpeg\"\n",
    "\n",
    "def resolve_resource(relative_path: str) -> Path:\n",
    "    path_candidate = Path(relative_path)\n",
    "    if path_candidate.is_absolute():\n",
    "        if path_candidate.exists():\n",
    "            return path_candidate\n",
    "        raise FileNotFoundError(f\"File not found: {path_candidate}\")\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for base in [cwd] + list(cwd.parents):\n",
    "        candidate = base / relative_path\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Unable to locate '{relative_path}'. Start the notebook from the project root or ensure the file exists.\"\n",
    "    )\n",
    "\n",
    "def load_image_base64(relative_path: str):\n",
    "    resolved_path = resolve_resource(relative_path)\n",
    "    with resolved_path.open(\"rb\") as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    return base64.b64encode(image_bytes).decode(\"utf-8\"), resolved_path\n",
    "\n",
    "# Load first vacation image\n",
    "image_data, image_file = load_image_base64(IMAGE_RELATIVE_PATH)\n",
    "print(f\"\\nAnalyzing: {image_file}\")\n",
    "\n",
    "# Build payload with base64-encoded image\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe this vacation photo. What location might this be? What activities or experiences does it suggest?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_data}\",\n",
    "                        \"detail\": \"high\"  # Use high detail for better analysis\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 400,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "# Build GPT-4o endpoint\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "gpt4o_url = f\"{endpoint}/openai/deployments/{gpt4o_deployment}/chat/completions?api-version={api_version}\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": os.environ['DIAL_API_KEY']\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(gpt4o_url, headers=headers, json=payload, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "\n",
    "    print(\"\\nGPT-4o Analysis:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['choices'][0]['message']['content'])\n",
    "\n",
    "    usage = result.get('usage', {})\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Token Usage:\")\n",
    "    print(f\"  Input tokens: {usage.get('prompt_tokens', 0)}\")\n",
    "    print(f\"  Output tokens: {usage.get('completion_tokens', 0)}\")\n",
    "    print(f\"  Total: {usage.get('total_tokens', 0)}\")\n",
    "    print(\"\\n High detail image analysis uses more tokens but provides richer descriptions\")\n",
    "\n",
    "    # Optional: Analyze second image\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Analyzing second vacation photo...\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    image_data2, image_file_2 = load_image_base64(IMAGE_RELATIVE_PATH_2)\n",
    "\n",
    "    payload2 = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this vacation scene. Compare the mood and setting to a beach destination.\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data2}\", \"detail\": \"low\"}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "\n",
    "    response2 = requests.post(gpt4o_url, headers=headers, json=payload2, timeout=60)\n",
    "    result2 = response2.json()\n",
    "\n",
    "    print(f\"\\nAnalyzing: {image_file_2}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result2['choices'][0]['message']['content'])\n",
    "\n",
    "    usage2 = result2.get('usage', {})\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Token Comparison (detail='low' vs detail='high'):\")\n",
    "    print(f\"  Image 1 (high): {usage.get('prompt_tokens', 0)} input tokens\")\n",
    "    print(f\"  Image 2 (low):  {usage2.get('prompt_tokens', 0)} input tokens\")\n",
    "    print(f\"  Savings: ~{usage.get('prompt_tokens', 0) - usage2.get('prompt_tokens', 0)} tokens with 'low' detail\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n Error: {e}\")\n",
    "    if hasattr(e, 'response') and e.response is not None:\n",
    "        try:\n",
    "            error_detail = e.response.json()\n",
    "            print(f\"Details: {error_detail}\")\n",
    "        except:\n",
    "            print(f\"Response: {e.response.text[:500]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Key Takeaways:\")\n",
    "print(\"=\" * 70)\n",
    "print(\" GPT-4o supports vision; GPT-5 is text-only\")\n",
    "print(\" 'detail': 'high' ‚Üí better analysis, more tokens\")\n",
    "print(\" 'detail': 'low' ‚Üí faster, cheaper, good for classification\")\n",
    "print(\" Images can be base64-encoded or public URLs\")\n",
    "print(\" Remove images from history after analysis to save tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645afd0c24455f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Prompt Patterns\n",
    "\n",
    "### Theory: Few-Shot Learning\n",
    "\n",
    "**Pattern:** Show 3-5 examples before actual query.\n",
    "\n",
    "**Benefits:**\n",
    "- Teaches output format without complex instructions\n",
    "- Improves consistency\n",
    "- Anchors domain-specific tone\n",
    "\n",
    "**Prompt Design:** \"Right altitude\" system prompts - specific enough for reliable heuristics, flexible enough to generalize. Use `<instructions>`, `<examples>` sections.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 10.1: Few-Shot SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544d2eebf984f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Sentiment analysis with specific format\n",
    "print(\"=\" * 70)\n",
    "print(\"Few-Shot Prompting Demo - Sentiment Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Zero-shot (no examples)\n",
    "zero_shot_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Analyze the sentiment of product reviews.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Review: This laptop exceeded my expectations! The battery lasts all day and it's super fast.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_completion_tokens\": 100,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "print(\"\\n1. ZERO-SHOT (no examples):\")\n",
    "result = client.call(zero_shot_payload)\n",
    "print(f\"Response: {result['data']['choices'][0]['message']['content']}\")\n",
    "\n",
    "# Few-shot (with examples)\n",
    "few_shot_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Analyze product review sentiment and provide a structured response.\"\n",
    "        },\n",
    "        # Example 1\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Review: The headphones broke after one week. Terrible quality.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"SENTIMENT: Negative\\nSCORE: 1/5\\nKEY_ISSUES: [durability, quality]\\nRECOMMENDATION: Not recommended\"\n",
    "        },\n",
    "        # Example 2\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Review: Decent product for the price. Works as expected.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"SENTIMENT: Neutral\\nSCORE: 3/5\\nKEY_ISSUES: []\\nRECOMMENDATION: Acceptable for budget-conscious buyers\"\n",
    "        },\n",
    "        # Example 3\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Review: Absolutely love it! Best purchase I've made this year.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"SENTIMENT: Positive\\nSCORE: 5/5\\nKEY_ISSUES: []\\nRECOMMENDATION: Highly recommended\"\n",
    "        },\n",
    "        # Actual query\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Review: This laptop exceeded my expectations! The battery lasts all day and it's super fast.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_completion_tokens\": 100,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "print(\"\\n2. FEW-SHOT (3 examples provided):\")\n",
    "result = client.call(few_shot_payload)\n",
    "print(f\"Response:\\n{result['data']['choices'][0]['message']['content']}\")\n",
    "\n",
    "# Example 2: Data transformation\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"Few-Shot Prompting Demo - Data Transformation\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "transform_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Transform natural language into SQL queries.\"\n",
    "        },\n",
    "        # Example 1\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Show me all users who signed up last month\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"SELECT * FROM users WHERE signup_date >= DATE_SUB(CURRENT_DATE, INTERVAL 1 MONTH) AND signup_date < CURRENT_DATE;\"\n",
    "        },\n",
    "        # Example 2\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Find the top 5 products by revenue\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"SELECT product_id, product_name, SUM(price * quantity) as total_revenue FROM orders GROUP BY product_id, product_name ORDER BY total_revenue DESC LIMIT 5;\"\n",
    "        },\n",
    "        # Example 3\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Count active users from each country\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"SELECT country, COUNT(*) as active_users FROM users WHERE status = 'active' GROUP BY country ORDER BY active_users DESC;\"\n",
    "        },\n",
    "        # Actual query\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Show me orders over $1000 from this week with customer names\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_completion_tokens\": 150,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "print(\"\\nQuery: 'Show me orders over $1000 from this week with customer names'\")\n",
    "result = client.call(transform_payload)\n",
    "print(f\"\\nGenerated SQL:\\n{result['data']['choices'][0]['message']['content']}\")\n",
    "\n",
    "# Example 3: Custom formatting\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"Few-Shot Prompting Demo - Custom Format\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "format_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Convert meeting notes into structured action items.\"\n",
    "        },\n",
    "        # Example\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Sarah mentioned we need to update the documentation and John will review the pull request by Friday.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\"[ ] @Sarah - Update documentation\n",
    "    Priority: Medium\n",
    "    Due: TBD\n",
    "    \n",
    "[ ] @John - Review pull request\n",
    "    Priority: High\n",
    "    Due: Friday\"\"\"\n",
    "        },\n",
    "        # Actual query\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Emily said she'll prepare the presentation for next Tuesday's client meeting. Mike needs to send the proposal by tomorrow and asked Lisa to help with the budget section.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_completion_tokens\": 200,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "print(\"\\nInput: Meeting notes\")\n",
    "result = client.call(format_payload)\n",
    "print(f\"\\nFormatted Action Items:\\n{result['data']['choices'][0]['message']['content']}\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"Key Benefits of Few-Shot Prompting:\")\n",
    "print(\"   More consistent output format\")\n",
    "print(\"   Better adherence to specific patterns\")\n",
    "print(\"   Reduced need for detailed instructions\")\n",
    "print(\"   Improved accuracy for domain-specific tasks\")\n",
    "print(f\"{'=' * 70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72c54746f6930a",
   "metadata": {},
   "source": [
    "### Demo 10.2: Context Management\n",
    "\n",
    "Long conversations with summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc7f0d994cfd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a long conversation with context management\n",
    "def count_tokens_estimate(text):\n",
    "    \"\"\"Rough token estimation: ~4 characters per token.\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "def summarize_conversation(messages):\n",
    "    \"\"\"Use the model to summarize old conversation turns.\"\"\"\n",
    "    summary_payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"developer\",\n",
    "                \"content\": \"Summarize the following conversation concisely, preserving key information.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Conversation:\\n\" + \"\\n\".join([\n",
    "                    f\"{msg['role']}: {msg['content']}\" for msg in messages\n",
    "                ])\n",
    "            }\n",
    "        ],\n",
    "        \"max_completion_tokens\": 200,\n",
    "        \"reasoning_effort\": \"low\"\n",
    "    }\n",
    "    result = client.call(summary_payload)\n",
    "    return result['data']['choices'][0]['message']['content']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Context Management Demo\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Start a conversation\n",
    "conversation = [\n",
    "    {\"role\": \"developer\", \"content\": \"You are a helpful coding assistant.\"}\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    \"What is Python?\",\n",
    "    \"How do I create a list in Python?\",\n",
    "    \"Can you explain list comprehensions?\",\n",
    "    \"What's the difference between a list and a tuple?\",\n",
    "    \"How do I sort a list?\",\n",
    "]\n",
    "\n",
    "MAX_CONTEXT_TOKENS = 500  # Simulate a small context limit\n",
    "\n",
    "print(f\"\\nContext limit: {MAX_CONTEXT_TOKENS} tokens\")\n",
    "print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    # Add user question\n",
    "    conversation.append({\"role\": \"user\", \"content\": question})\n",
    "    \n",
    "    # Estimate current context size\n",
    "    context_text = json.dumps(conversation)\n",
    "    estimated_tokens = count_tokens_estimate(context_text)\n",
    "    \n",
    "    print(f\"Turn {i}: {question}\")\n",
    "    print(f\"  Current context: ~{estimated_tokens} tokens\")\n",
    "    \n",
    "    # Check if we need to compact context\n",
    "    if estimated_tokens > MAX_CONTEXT_TOKENS:\n",
    "        print(f\"  Ô∏è  Context exceeds limit! Applying compaction...\")\n",
    "        \n",
    "        # Keep system message and last 2 turns, summarize the rest\n",
    "        system_msg = conversation[0]\n",
    "        messages_to_summarize = conversation[1:-1]  # Skip system and current question\n",
    "        \n",
    "        if len(messages_to_summarize) > 0:\n",
    "            print(f\"  Summarizing {len(messages_to_summarize)} old messages...\")\n",
    "            summary = summarize_conversation(messages_to_summarize)\n",
    "            \n",
    "            # Rebuild conversation with summary\n",
    "            conversation = [\n",
    "                system_msg,\n",
    "                {\"role\": \"user\", \"content\": f\"[Previous conversation summary: {summary}]\"},\n",
    "                conversation[-1]  # Current question\n",
    "            ]\n",
    "            \n",
    "            new_tokens = count_tokens_estimate(json.dumps(conversation))\n",
    "            print(f\"   Context reduced to ~{new_tokens} tokens\")\n",
    "            print(f\"  Summary generated: {summary}\")\n",
    "    \n",
    "    # Get response\n",
    "    payload = {\n",
    "        \"messages\": conversation,\n",
    "        \"max_completion_tokens\": 150,\n",
    "        \"reasoning_effort\": \"low\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = client.call(payload)\n",
    "        response = result['data']['choices'][0]['message']['content']\n",
    "        \n",
    "        # Validate response is not empty\n",
    "        if not response or response.strip() == '':\n",
    "            print(f\"\\n  Ô∏è  WARNING: Empty response received!\")\n",
    "            print(f\"  Finish reason: {result['data']['choices'][0].get('finish_reason', 'unknown')}\")\n",
    "            response = \"[Empty response from API]\"\n",
    "        \n",
    "        # Add response to conversation\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        # Show FULL response\n",
    "        print(f\"\\n  Response:\")\n",
    "        for line in response.split('\\n'):\n",
    "            print(f\"    {line}\")\n",
    "        print(f\"\\n  Messages in conversation: {len(conversation)}\")\n",
    "        print(f\"  Token usage: {result['data']['usage']['total_tokens']} tokens\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n   ERROR: {e}\")\n",
    "        print(f\"  Request failed - check your API configuration\")\n",
    "        break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"{'=' * 70}\")\n",
    "print(\"Context Management Strategies Demonstrated:\")\n",
    "print(\"  1. Token estimation before API calls\")\n",
    "print(\"  2. Automatic summarization of old turns\")\n",
    "print(\"  3. Sliding window (keeping recent messages)\")\n",
    "print(\"  4. System message preservation\")\n",
    "print(f\"{'=' * 70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b349995c89335d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: Model Comparison\n",
    "\n",
    "### Theory: Choosing the Right Model\n",
    "\n",
    "**GPT-5 (Reasoning):**\n",
    "-  Complex logic, debugging, math proofs\n",
    "-  Reasoning tokens reveal thinking process\n",
    "-  Slower, more expensive, no vision\n",
    "\n",
    "**GPT-4o (Standard):**\n",
    "-  Fast, vision support, temperature control\n",
    "-  Cheaper for high-volume tasks\n",
    "-  No reasoning phase, less reliable on complex logic\n",
    "\n",
    "**When to Use:** GPT-5 for hard problems, GPT-4o for speed/vision/volume.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 11.1: Parameter Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8c5a5964cc703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "api_key = os.environ['DIAL_API_KEY']\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'api-key': api_key,\n",
    "}\n",
    "\n",
    "prompt_text = \"Explain what a REST API is in one paragraph.\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: GPT-5 vs GPT-4o Parameter Differences\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# GPT-5 Request (Reasoning Model)\n",
    "print(\"\\n1Ô∏è‚É£  GPT-5 (Reasoning Model)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "gpt5_deployment = os.environ['DIAL_DEPLOYMENT']\n",
    "gpt5_url = f\"{endpoint}/openai/deployments/{gpt5_deployment}/chat/completions?api-version={api_version}\"\n",
    "\n",
    "gpt5_payload = {\n",
    "    'messages': [\n",
    "        {'role': 'developer', 'content': 'You are a technical writer.'},\n",
    "        {'role': 'user', 'content': prompt_text}\n",
    "    ],\n",
    "    'max_completion_tokens': 200,\n",
    "    'reasoning_effort': 'medium'\n",
    "}\n",
    "\n",
    "print(f\"Model: {gpt5_deployment}\")\n",
    "print(\"Parameters used:\")\n",
    "print(f\"  ‚Ä¢ max_completion_tokens: 200\")\n",
    "print(f\"  ‚Ä¢ reasoning_effort: medium\")\n",
    "print(f\"  ‚Ä¢ role: developer (not system)\")\n",
    "\n",
    "try:\n",
    "    response = requests.post(gpt5_url, headers=headers, json=gpt5_payload, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    \n",
    "    print(f\"\\n‚úì Response:\")\n",
    "    print(result['choices'][0]['message']['content'])\n",
    "    print(f\"\\nüìä Token usage: {result['usage']['total_tokens']} total\")\n",
    "    if 'completion_tokens_details' in result['usage']:\n",
    "        reasoning = result['usage']['completion_tokens_details'].get('reasoning_tokens', 0)\n",
    "        if reasoning:\n",
    "            print(f\"   Reasoning tokens: {reasoning}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Error: {e}\")\n",
    "\n",
    "# GPT-4o Request (Standard Model)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2Ô∏è‚É£  GPT-4o (Standard Model)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "gpt4o_deployment = os.environ['GPT4O_DEPLOYMENT']\n",
    "gpt4o_url = f\"{endpoint}/openai/deployments/{gpt4o_deployment}/chat/completions?api-version={api_version}\"\n",
    "\n",
    "gpt4o_payload = {\n",
    "    'messages': [\n",
    "        {'role': 'system', 'content': 'You are a technical writer.'},\n",
    "        {'role': 'user', 'content': prompt_text}\n",
    "    ],\n",
    "    'max_tokens': 200,\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 0.9\n",
    "}\n",
    "\n",
    "print(f\"Model: {gpt4o_deployment}\")\n",
    "print(\"Parameters used:\")\n",
    "print(f\"  ‚Ä¢ max_tokens: 200 (not max_completion_tokens)\")\n",
    "print(f\"  ‚Ä¢ temperature: 0.7\")\n",
    "print(f\"  ‚Ä¢ top_p: 0.9\")\n",
    "print(f\"  ‚Ä¢ role: system (not developer)\")\n",
    "\n",
    "try:\n",
    "    response = requests.post(gpt4o_url, headers=headers, json=gpt4o_payload, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    \n",
    "    print(f\"\\n‚úì Response:\")\n",
    "    print(result['choices'][0]['message']['content'])\n",
    "    print(f\"\\nüìä Token usage: {result['usage']['total_tokens']} total\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY DIFFERENCES:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nGPT-5 (Reasoning):\")\n",
    "print(\"  ‚úì max_completion_tokens, reasoning_effort\")\n",
    "print(\"  ‚úó NO temperature, top_p, max_tokens\")\n",
    "print(\"\\nGPT-4o (Standard):\")\n",
    "print(\"  ‚úì max_tokens, temperature, top_p\")\n",
    "print(\"  ‚úó NO reasoning_effort, max_completion_tokens\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0f9e4d92ec12e",
   "metadata": {},
   "source": [
    "### Demo 11.2: Reasoning Quality\n",
    "\n",
    "Complex logic with different efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc6b5640f70725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "api_key = os.environ['DIAL_API_KEY']\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'api-key': api_key,\n",
    "}\n",
    "\n",
    "# Complex reasoning problem\n",
    "problem = \"\"\"A snail is at the bottom of a 20-foot well. Each day it climbs up 3 feet, \n",
    "but each night it slides down 2 feet. On which day will the snail reach the top of the well?\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: Reasoning Quality - GPT-5 vs GPT-4o\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nProblem: {problem}\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# GPT-5 with HIGH reasoning effort\n",
    "print(\"\\n1Ô∏è‚É£  GPT-5 with reasoning_effort='high'\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "gpt5_url = f\"{endpoint}/openai/deployments/{os.environ['DIAL_DEPLOYMENT']}/chat/completions?api-version={api_version}\"\n",
    "\n",
    "gpt5_payload = {\n",
    "    'messages': [\n",
    "        {'role': 'developer', 'content': 'You are a logical reasoning expert. Show your work step by step.'},\n",
    "        {'role': 'user', 'content': problem}\n",
    "    ],\n",
    "    'max_completion_tokens': 500,\n",
    "    'reasoning_effort': 'high'\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "try:\n",
    "    response = requests.post(gpt5_url, headers=headers, json=gpt5_payload, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    gpt5_time = time.time() - start\n",
    "    \n",
    "    gpt5_answer = result['choices'][0]['message']['content']\n",
    "    gpt5_tokens = result['usage']['total_tokens']\n",
    "    gpt5_reasoning_tokens = result['usage'].get('completion_tokens_details', {}).get('reasoning_tokens', 0)\n",
    "    \n",
    "    print(f\"Response:\\n{gpt5_answer}\")\n",
    "    print(f\"\\nüìä Stats:\")\n",
    "    print(f\"   Total tokens: {gpt5_tokens}\")\n",
    "    print(f\"   Reasoning tokens: {gpt5_reasoning_tokens}\")\n",
    "    print(f\"   Time: {gpt5_time:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")\n",
    "    gpt5_answer = None\n",
    "\n",
    "# GPT-4o with temperature for comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2Ô∏è‚É£  GPT-4o with temperature=0.7\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "gpt4o_url = f\"{endpoint}/openai/deployments/{os.environ['GPT4O_DEPLOYMENT']}/chat/completions?api-version={api_version}\"\n",
    "\n",
    "gpt4o_payload = {\n",
    "    'messages': [\n",
    "        {'role': 'system', 'content': 'You are a logical reasoning expert. Show your work step by step.'},\n",
    "        {'role': 'user', 'content': problem}\n",
    "    ],\n",
    "    'max_tokens': 500,\n",
    "    'temperature': 0.7\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "try:\n",
    "    response = requests.post(gpt4o_url, headers=headers, json=gpt4o_payload, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    gpt4o_time = time.time() - start\n",
    "    \n",
    "    gpt4o_answer = result['choices'][0]['message']['content']\n",
    "    gpt4o_tokens = result['usage']['total_tokens']\n",
    "    \n",
    "    print(f\"Response:\\n{gpt4o_answer}\")\n",
    "    print(f\"\\nüìä Stats:\")\n",
    "    print(f\"   Total tokens: {gpt4o_tokens}\")\n",
    "    print(f\"   Time: {gpt4o_time:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")\n",
    "    gpt4o_answer = None\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n Key Observations:\")\n",
    "print(\"  ‚Ä¢ GPT-5 allocates reasoning tokens for deeper analysis\")\n",
    "print(\"  ‚Ä¢ GPT-5 may take longer but provides more thorough reasoning\")\n",
    "print(\"  ‚Ä¢ GPT-4o is faster but doesn't have dedicated reasoning phase\")\n",
    "print(\"\\n Correct answer is: Day 18\")\n",
    "print(\"   (On day 18, the snail climbs 3 feet and reaches 20 feet before sliding back)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561623aeb6b32139",
   "metadata": {},
   "source": [
    "### Demo 11.3: Cost vs Speed Trade-offs\n",
    "\n",
    "Token, latency, cost comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9ca1b775e32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "api_key = os.environ['DIAL_API_KEY']\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'api-key': api_key,\n",
    "}\n",
    "\n",
    "# Simple task that doesn't need deep reasoning\n",
    "simple_task = \"Write a professional email subject line for a meeting reminder.\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: Cost & Speed - When to Use Which Model\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTask: {simple_task}\")\n",
    "print(\"\\nThis is a SIMPLE task. Let's compare cost and speed.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Test GPT-5 with LOW reasoning (cheapest)\n",
    "print(\"\\n1Ô∏è‚É£  GPT-5 with reasoning_effort='low'\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "gpt5_url = f\"{endpoint}/openai/deployments/{os.environ['DIAL_DEPLOYMENT']}/chat/completions?api-version={api_version}\"\n",
    "\n",
    "gpt5_payload = {\n",
    "    'messages': [\n",
    "        {'role': 'developer', 'content': 'You write professional emails.'},\n",
    "        {'role': 'user', 'content': simple_task}\n",
    "    ],\n",
    "    'max_completion_tokens': 50,\n",
    "    'reasoning_effort': 'low'\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "try:\n",
    "    response = requests.post(gpt5_url, headers=headers, json=gpt5_payload, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "    print(f\"\\nüìä Stats:\")\n",
    "    print(f\"   Tokens: {result['usage']['total_tokens']}\")\n",
    "    print(f\"   Time: {elapsed:.2f}s\")\n",
    "    print(f\"   Est. cost: ${result['usage']['total_tokens'] * 0.000005:.6f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'model': 'GPT-5 (low)',\n",
    "        'tokens': result['usage']['total_tokens'],\n",
    "        'time': elapsed,\n",
    "        'cost': result['usage']['total_tokens'] * 0.000005\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")\n",
    "\n",
    "# Test GPT-5 with HIGH reasoning (expensive, overkill for simple task)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2Ô∏è‚É£  GPT-5 with reasoning_effort='high' (OVERKILL)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "gpt5_high_payload = {\n",
    "    'messages': [\n",
    "        {'role': 'developer', 'content': 'You write professional emails.'},\n",
    "        {'role': 'user', 'content': simple_task}\n",
    "    ],\n",
    "    'max_completion_tokens': 50,\n",
    "    'reasoning_effort': 'high'\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "try:\n",
    "    response = requests.post(gpt5_url, headers=headers, json=gpt5_high_payload, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "    print(f\"\\nüìä Stats:\")\n",
    "    print(f\"   Tokens: {result['usage']['total_tokens']}\")\n",
    "    reasoning_tokens = result['usage'].get('completion_tokens_details', {}).get('reasoning_tokens', 0)\n",
    "    print(f\"   Reasoning tokens: {reasoning_tokens} (wasted for simple task)\")\n",
    "    print(f\"   Time: {elapsed:.2f}s\")\n",
    "    print(f\"   Est. cost: ${result['usage']['total_tokens'] * 0.000005:.6f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'model': 'GPT-5 (high)',\n",
    "        'tokens': result['usage']['total_tokens'],\n",
    "        'time': elapsed,\n",
    "        'cost': result['usage']['total_tokens'] * 0.000005\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")\n",
    "\n",
    "# Test GPT-4o (faster for simple tasks)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3Ô∏è‚É£  GPT-4o (Optimized for simple tasks)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "gpt4o_url = f\"{endpoint}/openai/deployments/{os.environ['GPT4O_DEPLOYMENT']}/chat/completions?api-version={api_version}\"\n",
    "\n",
    "gpt4o_payload = {\n",
    "    'messages': [\n",
    "        {'role': 'system', 'content': 'You write professional emails.'},\n",
    "        {'role': 'user', 'content': simple_task}\n",
    "    ],\n",
    "    'max_tokens': 50,\n",
    "    'temperature': 0.7\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "try:\n",
    "    response = requests.post(gpt4o_url, headers=headers, json=gpt4o_payload, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "    print(f\"\\nüìä Stats:\")\n",
    "    print(f\"   Tokens: {result['usage']['total_tokens']}\")\n",
    "    print(f\"   Time: {elapsed:.2f}s\")\n",
    "    print(f\"   Est. cost: ${result['usage']['total_tokens'] * 0.000002:.6f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'model': 'GPT-4o',\n",
    "        'tokens': result['usage']['total_tokens'],\n",
    "        'time': elapsed,\n",
    "        'cost': result['usage']['total_tokens'] * 0.000002\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä COMPARISON TABLE:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n{'Model':<20} {'Tokens':<10} {'Time (s)':<12} {'Est. Cost':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    for r in results:\n",
    "        print(f\"{r['model']:<20} {r['tokens']:<10} {r['time']:<12.2f} ${r['cost']:<11.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" RECOMMENDATIONS:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n Use GPT-5 (high reasoning) for:\")\n",
    "print(\"   ‚Ä¢ Complex logic problems\")\n",
    "print(\"   ‚Ä¢ Code debugging and optimization\")\n",
    "print(\"   ‚Ä¢ Mathematical proofs\")\n",
    "print(\"   ‚Ä¢ Multi-step planning\")\n",
    "print(\"\\n Use GPT-5 (low reasoning) for:\")\n",
    "print(\"   ‚Ä¢ Balanced quality and cost\")\n",
    "print(\"   ‚Ä¢ Moderate complexity tasks\")\n",
    "print(\"\\n Use GPT-4o for:\")\n",
    "print(\"   ‚Ä¢ Simple content generation\")\n",
    "print(\"   ‚Ä¢ Quick responses\")\n",
    "print(\"   ‚Ä¢ High-volume, low-complexity tasks\")\n",
    "print(\"   ‚Ä¢ When speed matters more than deep reasoning\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea783b72756f6b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workshop Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Topic | Core Concepts |\n",
    "|-------|---------------|\n",
    "| **API Basics** | Stateless calls, token billing, finish_reason validation |\n",
    "| **Models** | GPT-5 reasoning vs GPT-4o parameters |\n",
    "| **Conversations** | Resend full history, sliding windows |\n",
    "| **Tokens** | Prompt + completion < limit, cap completions |\n",
    "| **Structured** | JSON mode for syntax, schemas for contracts |\n",
    "| **Tools** | Define schemas, execute, return results |\n",
    "| **Streaming** | Real-time SSE for better UX |\n",
    "| **Vision** | GPT-4o images with detail control |\n",
    "| **Prompting** | Few-shot teaches patterns efficiently |\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "**Before deployment:**\n",
    "-  Validate `finish_reason` before using responses\n",
    "-  Set `max_completion_tokens` to prevent cost overruns\n",
    "-  Log token usage per request\n",
    "-  Implement exponential backoff for rate limits (429 errors)\n",
    "-  Version prompts and schemas in source control\n",
    "-  Test edge cases: truncation, timeouts, refusals\n",
    "\n",
    "**Cost optimization:**\n",
    "- Prune conversation history aggressively\n",
    "- Cache repeated system prompts\n",
    "- Choose appropriate reasoning effort per request\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Build domain-specific tools (DB queries, API calls)\n",
    "2. Combine techniques: few-shot + tools + structured outputs\n",
    "3. Implement context compaction with summarization\n",
    "4. Test vision workflows with images\n",
    "5. Measure token costs across reasoning efforts\n",
    "\n",
    "**Spring AI integration:** Map these patterns to `ChatClient` APIs, implement tool calling with `@Tool` annotations, add Actuator metrics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf2d1037942dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = client.usage_summary()\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382fa1ba2e9eea2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 12: Context Window Strategies\n",
    "\n",
    "### Theory: Lost-in-the-Middle & Compaction\n",
    "\n",
    "**Context Quality:**\n",
    "- Quality drops beyond 50-55% of context limit\n",
    "- \"Lost-in-the-middle\" effect: model misses details buried in long contexts\n",
    "- Keep high-signal content at top and bottom\n",
    "\n",
    "**Compaction Strategies:**\n",
    "- Summarization: compress old turns using the model itself\n",
    "- Sliding window: keep last N turns verbatim\n",
    "- External memory: store details outside, retrieve just-in-time via tools\n",
    "\n",
    "**Impact:** Summarization alone lifts success by ~29%; with external memory: ~39% improvement, 84% token reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 12.1: Context Compaction with Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e2069b97b8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context compaction demo\n",
    "def summarize_conversation(messages):\n",
    "    '''Compress old conversation turns into a summary.'''\n",
    "    summary_payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"developer\", \"content\": \"Summarize the following conversation concisely, preserving key information.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Conversation:\\n\" + \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])}\n",
    "        ],\n",
    "        \"max_completion_tokens\": 200,\n",
    "        \"reasoning_effort\": \"low\"\n",
    "    }\n",
    "    result = client.call(summary_payload)\n",
    "    return result['data']['choices'][0]['message']['content']\n",
    "\n",
    "# Simulate long conversation\n",
    "conversation = [\n",
    "    {\"role\": \"developer\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Python is a high-level, interpreted programming language known for its simplicity and readability.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I create a list?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Use square brackets: my_list = [1, 2, 3] or list() constructor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about dictionaries?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Dictionaries use curly braces: my_dict = {'key': 'value'}\"}\n",
    "]\n",
    "\n",
    "print(\"Original conversation: {} messages, ~{} chars\".format(len(conversation), sum(len(str(m)) for m in conversation)))\n",
    "\n",
    "# Summarize old turns (keep system + last 2)\n",
    "old_turns = conversation[1:-2]\n",
    "summary = summarize_conversation(old_turns)\n",
    "\n",
    "print(f\"\\nSummary of old turns:\\n{summary}\")\n",
    "\n",
    "# Rebuild with summary\n",
    "compacted = [\n",
    "    conversation[0],  # system\n",
    "    {\"role\": \"user\", \"content\": f\"[Previous: {summary}]\"},\n",
    "    conversation[-2],  # recent messages\n",
    "    conversation[-1]\n",
    "]\n",
    "\n",
    "print(f\"\\nCompacted conversation: {len(compacted)} messages, ~{sum(len(str(m)) for m in compacted)} chars\")\n",
    "print(f\"Token savings: ~{(1 - sum(len(str(m)) for m in compacted) / sum(len(str(m)) for m in conversation)) * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e99edaaf57429",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 13: Parameter Tuning\n",
    "\n",
    "### Theory: Temperature vs Reasoning Effort\n",
    "\n",
    "**GPT-4o (Standard) Parameters:**\n",
    "- `temperature` (0.0-2.0): Higher = more random/creative\n",
    "- `top_p` (0.0-1.0): Nucleus sampling, restricts token pool\n",
    "- Use ONE of temperature or top_p, not both\n",
    "\n",
    "**GPT-5 (Reasoning) Parameters:**\n",
    "- `reasoning_effort` (low/medium/high): Allocates thinking tokens\n",
    "- NO temperature/top_p support\n",
    "- Trade-off: accuracy vs cost/latency\n",
    "\n",
    "**When to tune:** Deterministic tasks ‚Üí low temperature/effort. Creative tasks ‚Üí high temperature (GPT-4o only).\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 13.1: Temperature Impact on Creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93cd8642a326dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare temperature settings on GPT-4o\n",
    "endpoint = os.environ['DIAL_API_ENDPOINT']\n",
    "api_version = os.environ['DIAL_API_VERSION']\n",
    "gpt4o_url = f\"{endpoint}/openai/deployments/{os.environ['GPT4O_DEPLOYMENT']}/chat/completions?api-version={api_version}\"\n",
    "headers = {'Content-Type': 'application/json', 'api-key': os.environ['DIAL_API_KEY']}\n",
    "\n",
    "prompt = \"Write a creative company name for a coffee shop.\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Temperature Impact Demo (GPT-4o)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for temp in [0.0, 0.7, 1.5]:\n",
    "    payload = {\n",
    "        'messages': [\n",
    "            {'role': 'system', 'content': 'You are a creative brand consultant.'},\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        'max_tokens': 50,\n",
    "        'temperature': temp\n",
    "    }\n",
    "    \n",
    "    response = requests.post(gpt4o_url, headers=headers, json=payload, timeout=30)\n",
    "    result = response.json()\n",
    "    \n",
    "    print(f\"\\nTemperature {temp}:\")\n",
    "    print(f\"  {result['choices'][0]['message']['content'].strip()}\")\n",
    "\n",
    "print(\"\\n Notice: Higher temperature = more diverse/creative outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87afeb7595b73fb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 14: Error Handling & Reliability\n",
    "\n",
    "### Theory: Production Readiness\n",
    "\n",
    "**Always Check `finish_reason`:**\n",
    "- `stop`: Normal completion\n",
    "- `length`: Truncated (increase max_completion_tokens)\n",
    "- `content_filter`: Blocked by safety filters\n",
    "- `tool_calls`: Model wants to call a function\n",
    "\n",
    "**Rate Limits:**\n",
    "- HTTP 429: Too many requests\n",
    "- Headers: `x-ratelimit-remaining-requests`, `x-ratelimit-remaining-tokens`\n",
    "- Implement exponential backoff: 1s, 2s, 4s, 8s...\n",
    "\n",
    "**Logging:** Track request IDs, token usage, latency, errors for debugging.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 14.1: Handle Truncation and Rate Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e0cdef467644e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Demo 1: Detect truncation\n",
    "print(\"=\" * 70)\n",
    "print(\"Demo 1: Detecting Truncation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "truncated_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"developer\", \"content\": \"You are a technical writer.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain microservices architecture in detail with examples.\"}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 30,  # Too small!\n",
    "    \"reasoning_effort\": \"low\"\n",
    "}\n",
    "\n",
    "result = client.call(truncated_payload)\n",
    "choice = result['data']['choices'][0]\n",
    "\n",
    "print(f\"Response: {choice['message']['content'][:100]}...\")\n",
    "print(f\"\\nFinish reason: {choice['finish_reason']}\")\n",
    "\n",
    "if choice['finish_reason'] == 'length':\n",
    "    print(\"Ô∏è  TRUNCATED! Need to increase max_completion_tokens or reduce prompt.\")\n",
    "\n",
    "# Demo 2: Exponential backoff simulation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Demo 2: Exponential Backoff Pattern\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def call_with_retry(payload, max_retries=3):\n",
    "    '''Call API with exponential backoff on rate limits.'''\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = client.call(payload)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if '429' in str(e) and attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # 1s, 2s, 4s\n",
    "                print(f\"Rate limited! Waiting {wait_time}s before retry {attempt + 2}/{max_retries}...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "print(\"Retry pattern: 1s ‚Üí 2s ‚Üí 4s ‚Üí 8s...\")\n",
    "print(\" Always implement exponential backoff for production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8395c02d4fef0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 15: Just-in-Time Retrieval\n",
    "\n",
    "### Theory: Keep Prompts Lean\n",
    "\n",
    "**Problem:** Stuffing entire documents into prompts wastes tokens and reduces quality.\n",
    "\n",
    "**Solution:** Progressive disclosure\n",
    "1. Store lightweight identifiers (file paths, IDs)\n",
    "2. Surface summaries/metadata up front\n",
    "3. Let model request details via tool calls\n",
    "4. Load heavy data only when needed\n",
    "\n",
    "**Pattern:** `getFileList()` ‚Üí `getFileSummary(id)` ‚Üí `getFileContents(id)` only when necessary.\n",
    "\n",
    "---\n",
    "\n",
    "### Demo 15.1: Progressive Disclosure with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05a194b538d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a document database\n",
    "documents = {\n",
    "    \"doc1\": {\"title\": \"API Design Best Practices\", \"summary\": \"REST principles, versioning, error handling\", \"content\": \"... 5000 words ...\"},\n",
    "    \"doc2\": {\"title\": \"Database Optimization Guide\", \"summary\": \"Indexing, query optimization, caching strategies\", \"content\": \"... 8000 words ...\"},\n",
    "    \"doc3\": {\"title\": \"Kubernetes Deployment Patterns\", \"summary\": \"Rolling updates, blue-green, canary deployments\", \"content\": \"... 6000 words ...\"}\n",
    "}\n",
    "\n",
    "# Tool 1: List available documents\n",
    "def list_documents():\n",
    "    return [{\"id\": doc_id, \"title\": doc[\"title\"]} for doc_id, doc in documents.items()]\n",
    "\n",
    "# Tool 2: Get summary\n",
    "def get_document_summary(doc_id):\n",
    "    if doc_id in documents:\n",
    "        return {\"id\": doc_id, \"title\": documents[doc_id][\"title\"], \"summary\": documents[doc_id][\"summary\"]}\n",
    "    return {\"error\": \"Document not found\"}\n",
    "\n",
    "# Tool 3: Get full content (only when needed)\n",
    "def get_document_content(doc_id):\n",
    "    if doc_id in documents:\n",
    "        return {\"id\": doc_id, \"content\": documents[doc_id][\"content\"]}\n",
    "    return {\"error\": \"Document not found\"}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Just-in-Time Retrieval Pattern\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nStep 1: User asks question\")\n",
    "print(\"Query: 'How do I implement blue-green deployments?'\")\n",
    "\n",
    "print(\"\\nStep 2: Model calls list_documents()\")\n",
    "doc_list = list_documents()\n",
    "print(f\"Available: {[d['title'] for d in doc_list]}\")\n",
    "\n",
    "print(\"\\nStep 3: Model identifies relevant doc, calls get_document_summary('doc3')\")\n",
    "summary = get_document_summary(\"doc3\")\n",
    "print(f\"Summary: {summary['summary']}\")\n",
    "\n",
    "print(\"\\nStep 4: Model determines it needs full content, calls get_document_content('doc3')\")\n",
    "print(\"(Only NOW do we load the heavy 6000-word document into context)\")\n",
    "\n",
    "print(\"\\n Token savings: Only loaded 1 document instead of all 3!\")\n",
    "print(\"   Kept context focused and reduced cost by ~70%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
